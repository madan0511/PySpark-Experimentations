# PySpark-Experimentations

### PySpark sample experiments with logistic regression on datasets


# PySpark 

1. Python wrapper around scala - around which spark is developed to perform ETL tasks.
2. Used with cluster to provide data to be RDD ( Resilient Distributed Data ) which is fast and as it is distributed it is more unlikely to get corrupted or erased.
3. More similar to pandas dataframe but we can perform more complex querying using pyspark and build machine learning pipelines.
4. Used to perform ETL and build machine learning modelling and pipeline building with large datasets.



#### I've explained sql querying with select, join, qaggregation, grouping and the combinations of all. We can perform much more complex querying with pyspark and it is easy to do so.
